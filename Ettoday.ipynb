{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from tenacity import retry, stop_after_attempt\n",
    "import simplejson as json\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArticleMeta = namedtuple('ArticleMeta', ['url', 'date', 'time', 'category', 'title', 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsCrawler(object):\n",
    "    \n",
    "    def __init__(self, output_dir, total_days, start_date=datetime.date.today()):\n",
    "        \n",
    "        self.session = requests.Session()\n",
    "        self.headers = {\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,ja;q=0.6\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Referer\": \"https://www.google.com.tw/\",\n",
    "            \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) \"\\\n",
    "                          \"Chrome/69.0.3497.92 Safari/537.36\"\n",
    "        }\n",
    "        self.scroll_pause_time = 0.3\n",
    "        self.driver = webdriver.PhantomJS(executable_path='../phantomjs-2.1.1-linux-x86_64/bin/phantomjs')\n",
    "        self.output_dir = output_dir\n",
    "        self.total_days = total_days\n",
    "        self.start_date = start_date\n",
    "        self.newslinks = set()\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(3))\n",
    "    def get_bsObj(self, url):\n",
    "        \n",
    "        req = self.session.get(url, headers=self.headers)\n",
    "        if req.url != url:\n",
    "            return None\n",
    "        bsObj = BS(req.text, \"html.parser\")\n",
    "        return bsObj\n",
    "    \n",
    "    def get_bsObj_scroll_down(self, url):\n",
    "\n",
    "        self.driver.get(url)\n",
    "        # Get scroll height\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # Wait to load page\n",
    "            time.sleep(self.scroll_pause_time)\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "        return BS(self.driver.page_source, \"html.parser\")\n",
    "    \n",
    "    def date_generator(self):\n",
    "        \n",
    "        date = self.start_date\n",
    "        for _ in range(self.total_days):\n",
    "            yield str(date)\n",
    "            date = date - datetime.timedelta(days=1)\n",
    "    \n",
    "    # the functions below will be different for several news site.\n",
    "    def newslink_generator(self):\n",
    "        pass\n",
    "    \n",
    "    def bad_newspage_checker(self, bsObj):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EttodayNewsCrawler(NewsCrawler):\n",
    "    \n",
    "    def date_to_newslist_url(self, date):\n",
    "        \n",
    "        return \"https://www.ettoday.net/news/news-list-%s-0.htm\" % date\n",
    "    \n",
    "    def unfold_abbr_newslink(self, url):\n",
    "        \n",
    "        return \"https://www.ettoday.net\" + url\n",
    "    \n",
    "    def newslink_generator(self):\n",
    "        \n",
    "        ettoday_date_form = re.compile(\"[0-9]{4}/[0-9]{2}/[0-9]{2} [0-9]{2}:[0-9]{2}\")\n",
    "        \n",
    "        for date in self.date_generator():\n",
    "            newslist_url = self.date_to_newslist_url(date)\n",
    "            newslist = self.get_bsObj_scroll_down(newslist_url)\n",
    "            news_datetimes = newslist.find_all('span', class_='date', text=ettoday_date_form)\n",
    "\n",
    "            for news_datetime in news_datetimes:\n",
    "                newslinks = news_datetime.parent.a['href']\n",
    "                yield newslinks\n",
    "    \n",
    "    def parse_category(self, newspage):\n",
    "        return newspage.findAll('a', itemprop='item')[1].span.text\n",
    "    \n",
    "    def parse_title(self, newspage):\n",
    "        return newspage.find('h1', class_='title', itemprop='headline').text\n",
    "    \n",
    "    def parse_article(self, newspage):\n",
    "        paragraphs = []\n",
    "        for paragraph in newspage.find('article').find_all('p'):\n",
    "            if paragraph.findChild():\n",
    "                continue\n",
    "            else:\n",
    "                paragraphs.append(paragraph.text)\n",
    "        return '\\n'.join(paragraphs)\n",
    "    \n",
    "    def parse_date_time(self, newspage):\n",
    "        # workaround\n",
    "        datetime_string = newspage.find('time')['datetime'][:-6]\n",
    "        dt = datetime.datetime.strptime(datetime_string, \"%Y-%m-%dT%H:%M:%S\")\n",
    "        return str(dt.date()), str(dt.time())\n",
    "    \n",
    "    def is_valid_newspage(self, bsObj):\n",
    "        \n",
    "        if bsObj is None:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            if bsObj.find('em').text == '404錯誤':\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        except:\n",
    "            return True\n",
    "    \n",
    "    def saved_filename(self, url):\n",
    "        return url.split('/')[-1].split('.')[0] + '.json'\n",
    "    \n",
    "    def crawl_and_save(self):\n",
    "        \n",
    "        for newslink in self.newslink_generator():\n",
    "            \n",
    "            unfolded_newslink = self.unfold_abbr_newslink(newslink)\n",
    "            if unfolded_newslink in self.newslinks:\n",
    "                continue\n",
    "            else:\n",
    "                self.newslinks.add(unfolded_newslink)\n",
    "\n",
    "            page = self.get_bsObj(unfolded_newslink)\n",
    "            if not self.is_valid_newspage(page):\n",
    "                print('Invalid Page or Redirected Page:', unfolded_newslink)\n",
    "                continue\n",
    "            \n",
    "            print('Crawling News:', unfolded_newslink)\n",
    "            category = self.parse_category(page)\n",
    "            text = self.parse_article(page)\n",
    "            date_str, time_str = self.parse_date_time(page)\n",
    "            title = self.parse_title(page)\n",
    "            \n",
    "            article = ArticleMeta(\n",
    "                url=unfolded_newslink,\n",
    "                date=date_str,\n",
    "                time=time_str,\n",
    "                category=category,\n",
    "                title=title,\n",
    "                content=text\n",
    "            )\n",
    "            \n",
    "            output_dir_with_date = os.path.join(self.output_dir, date_str)\n",
    "            os.makedirs(output_dir_with_date ,exist_ok=True)\n",
    "            filename = os.path.join(output_dir_with_date, self.saved_filename(unfolded_newslink))\n",
    "            with open(filename, 'w+', encoding='utf-8') as f:\n",
    "                json.dump(article._asdict(), f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ettoday_crawler = EttodayNewsCrawler(\n",
    "    output_dir='news/ettoday',\n",
    "    total_days=1,\n",
    "    start_date=datetime.date(year=2019, month=11, day=29)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ettoday_crawler.crawl_and_save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
