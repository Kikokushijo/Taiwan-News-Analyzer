{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from tenacity import retry, stop_after_attempt\n",
    "import simplejson as json\n",
    "from selenium import webdriver\n",
    "\n",
    "from utils import ArticleMeta, NewsCrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetnNewsCrawler(NewsCrawler):\n",
    "    \n",
    "    # 650887 is a piece of news at 2018/12/10 01:10:00, I start to crawl\n",
    "    def __init__(self, output_dir, total_days, start_date=datetime.date.today(), start_id=630214):\n",
    "        super(SetnNewsCrawler, self).__init__(output_dir, total_days, start_date)\n",
    "        self.start_id = start_id\n",
    "        self.end_date = start_date - datetime.timedelta(days=total_days)\n",
    "    \n",
    "    def newslink_generator(self):\n",
    "        \n",
    "        news_prefix = \"https://www.setn.com/News.aspx?NewsID=\"\n",
    "        for news_id in range(self.start_id, 0, -1):\n",
    "            yield news_prefix + str(news_id)\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(0),\n",
    "           retry_error_callback=lambda x: None)\n",
    "    def parse_category(self, newspage):\n",
    "        return newspage.find('meta', property='article:section')['content']\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(0),\n",
    "           retry_error_callback=lambda x: None)\n",
    "    def parse_title(self, newspage):\n",
    "        return newspage.title.text.split('|')[0].strip()\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(0),\n",
    "           retry_error_callback=lambda x: None)\n",
    "    def parse_article(self, newspage):\n",
    "        paragraphs = []\n",
    "        for paragraph in newspage.find('article').find_all('p'):\n",
    "            if paragraph.findChild() or paragraph.attrs:\n",
    "                continue\n",
    "            else:\n",
    "                paragraphs.append(paragraph.text)\n",
    "        return '\\n'.join(paragraphs)\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(0),\n",
    "           retry_error_callback=lambda x: None)\n",
    "    def parse_date_time(self, newspage):\n",
    "        # workaround\n",
    "        datetime_string = newspage.find('meta', property='article:published_time')['content']\n",
    "        dt = datetime.datetime.strptime(datetime_string, \"%Y-%m-%dT%H:%M:%S\")\n",
    "        return str(dt.date()), str(dt.time())\n",
    "    \n",
    "    def is_valid_newspage(self, bsObj):\n",
    "        \n",
    "        if bsObj is None:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            if bsObj.findAll('img', src='/images/404.png'):\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        except:\n",
    "            return True\n",
    "    \n",
    "    def saved_filename(self, url):\n",
    "        return url.split('=')[-1]\n",
    "    \n",
    "    def crawl_and_save(self):\n",
    "        \n",
    "        num_old_news = 0\n",
    "        \n",
    "        for newslink in self.newslink_generator():\n",
    "\n",
    "            article = self.get_page_attribute_from_link(newslink)\n",
    "            \n",
    "            if article is None:\n",
    "                continue\n",
    "            \n",
    "            # check if there are continuous 100 piece of old news\n",
    "            if article.date <= str(self.end_date):\n",
    "                num_old_news += 1\n",
    "                if num_old_news >= 100:\n",
    "                    break\n",
    "            else:\n",
    "                num_old_news = 0\n",
    "        \n",
    "            self.save_article_meta(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setn_crawler = SetnNewsCrawler(output_dir='news/setn', total_days=1095, start_id=568884)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = setn_crawler.crawl_and_save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
