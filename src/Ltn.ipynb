{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from tenacity import retry, stop_after_attempt\n",
    "import simplejson as json\n",
    "from selenium import webdriver\n",
    "\n",
    "from utils import ArticleMeta, NewsCrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LtnNewsCrawler(NewsCrawler):\n",
    "\n",
    "    def date_newslist_generator(self, start_date, end_date, days=90):\n",
    "        \n",
    "        chunk_start_date = start_date\n",
    "        chunk_end_date = chunk_start_date - datetime.timedelta(days=days-1)\n",
    "        while chunk_start_date >= end_date:\n",
    "            \n",
    "            yield \"https://news.ltn.com.tw/search?keyword=&conditions=and&start_time=%s&end_time=%s\" % (chunk_end_date, chunk_start_date)\n",
    "            \n",
    "            chunk_end_date -= datetime.timedelta(days=days)\n",
    "            chunk_start_date -= datetime.timedelta(days=days)\n",
    "\n",
    "        return BS(self.driver.page_source, \"html.parser\")\n",
    "    \n",
    "    def newslink_generator(self):\n",
    "        \n",
    "        for newslist_url in self.date_newslist_generator(self.start_date, self.end_date):\n",
    "            \n",
    "            for page_num in itertools.count(1):\n",
    "                newslist_page = self.get_bsObj(newslist_url + \"&page=\" + str(page_num))\n",
    "                tits = newslist_page.findAll('a', class_='tit')\n",
    "                if not tits:\n",
    "                    break\n",
    "                else:\n",
    "                    for tit in tits:\n",
    "                        yield tit['href']\n",
    "\n",
    "    @retry(stop=stop_after_attempt(0),\n",
    "           retry_error_callback=lambda x: None)\n",
    "    def parse_category(self, newspage):\n",
    "        return newspage.find('title').text.split(' - ')[1]\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(0),\n",
    "           retry_error_callback=lambda x: None)\n",
    "    def parse_title(self, newspage):\n",
    "        return newspage.find('title').text.split(' - ')[0]\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(0),\n",
    "           retry_error_callback=lambda x: None)\n",
    "    def parse_article(self, newspage):\n",
    "        paragraphs = []\n",
    "        for paragraph in newspage.find('div', class_='text').findChildren(\"p\" , recursive=False):\n",
    "            if paragraph.findChild() or paragraph.has_attr('class'):\n",
    "                continue\n",
    "            else:\n",
    "                paragraphs.append(paragraph.text)\n",
    "        return '\\n'.join(paragraphs)\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(0),\n",
    "           retry_error_callback=lambda x: (None, None))\n",
    "    def parse_date_time(self, newspage):\n",
    "        # workaround\n",
    "        datetime_string = newspage.find('span', class_='time').text.strip()   \n",
    "        dt = datetime.datetime.strptime(datetime_string, \"%Y-%m-%d %H:%M:%S\")\n",
    "        return str(dt.date()), str(dt.time())\n",
    "\n",
    "    def is_valid_newspage(self, bsObj):\n",
    "        \n",
    "        if bsObj is None:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            if '錯誤' in bsObj.find('title').text:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        except:\n",
    "            return True\n",
    "    \n",
    "    def saved_filename(self, url):\n",
    "        return '-'.join(url.split('/')[-2:]) + '.json'\n",
    "    \n",
    "    def crawl_and_save(self):\n",
    "\n",
    "        for newslink in self.newslink_generator():\n",
    "            \n",
    "            article = self.get_page_attribute_from_link(newslink)\n",
    "            if article is None:\n",
    "                continue\n",
    "\n",
    "            self.save_article_meta(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = LtnNewsCrawler(output_dir='../news/ltn', total_days=1095)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.crawl_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('../logs/setn.txt', 'a') as f:\n",
    "    with contextlib.redirect_stdout(f):\n",
    "        setn_crawler.crawl_and_save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
