{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import concurrent.futures\n",
    "from functools import reduce\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import simplejson as json\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_filename = '../words/words.csv'\n",
    "word_segmentation_filename = '../words/bunshimondai.txt'\n",
    "blacklist_filename = '../words/black_list.txt'\n",
    "pluslist_filename = '../words/plus_list.txt'\n",
    "jieba_dict_filename = '../words/dict.txt.big'\n",
    "jieba.set_dictionary(jieba_dict_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(word_segmentation_filename, 'r') as f:\n",
    "    for line in f:\n",
    "        jieba.add_word(line.strip(), freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordlist_to_wordset(word_list):\n",
    "    words = []\n",
    "    for word_group in word_list:\n",
    "        if type(word_group) is str:\n",
    "            words.extend([c.strip() for c in word_group.split('/')])\n",
    "    return set(words)\n",
    "\n",
    "def build_tw_cn_dict(tw_word_list, cn_word_list):\n",
    "    \n",
    "    tw2cn = defaultdict(list)\n",
    "    cn2tw = defaultdict(list)\n",
    "    \n",
    "    for tw_word_group, cn_word_group in zip(tw_word_list, cn_word_list):\n",
    "        if type(tw_word_group) is str:\n",
    "            tw_words = [c.strip() for c in tw_word_group.split('/')]\n",
    "        else:\n",
    "            tw_words = [None]\n",
    "        \n",
    "        if type(cn_word_group) is str:\n",
    "            cn_words = [c.strip() for c in cn_word_group.split('/')]\n",
    "        else:\n",
    "            tw_words = [None]\n",
    "            \n",
    "        for tw_word in tw_words:\n",
    "            for cn_word in cn_words:\n",
    "                tw2cn[tw_word].append(cn_word)\n",
    "                cn2tw[cn_word].append(tw_word)\n",
    "    \n",
    "    return tw2cn, cn2tw\n",
    "\n",
    "def new_defined_wordset(tw_word_list, cn_word_list):\n",
    "    \n",
    "    wordset = set()\n",
    "    for tw_word_group, cn_word_group in zip(tw_word_list, cn_word_list):\n",
    "        if type(tw_word_group) is str and type(cn_word_group) is str:\n",
    "            tw_words = [c.strip() for c in tw_word_group.split('/')]\n",
    "            cn_words = set([c.strip() for c in cn_word_group.split('/')])\n",
    "            for cn_word in cn_words:\n",
    "                for tw_word in tw_words:\n",
    "                    if cn_word in tw_word:\n",
    "                        break\n",
    "                else:\n",
    "                    wordset.add(cn_word)\n",
    "    \n",
    "    wordset -= wordlist_to_wordset(tw_word_list)\n",
    "    return wordset\n",
    "\n",
    "def read_list_file(filename):\n",
    "    word_set = set()\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            word_set.add(line.strip())\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(words_filename)\n",
    "cn_word_list = df['cn_word'].tolist()\n",
    "tw_word_list = df['tw_word'].tolist()\n",
    "cn_word_set = wordlist_to_wordset(cn_word_list)\n",
    "tw_word_set = wordlist_to_wordset(tw_word_list)\n",
    "word_set = cn_word_set - tw_word_set - read_list_file(blacklist_filename) | read_list_file(pluslist_filename)\n",
    "tw2cn, cn2tw = build_tw_cn_dict(tw_word_list, cn_word_list)\n",
    "filenames = glob.glob('../news/*/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cn_words(article):\n",
    "    cn_word_count = Counter()\n",
    "    seg_list = jieba.cut(article, cut_all=False)\n",
    "    seg_counter = Counter(seg_list)\n",
    "    \n",
    "    for key in set(seg_counter) - word_set:\n",
    "        seg_counter[key] = 0\n",
    "    seg_counter = +seg_counter\n",
    "    \n",
    "    return seg_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cn_words_from_filename(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        article = json.load(f)\n",
    "        result = find_cn_words(article['content'])\n",
    "#         if result:\n",
    "#             print(article['url'], result)\n",
    "    split_filename = filename.split('/')\n",
    "    media = split_filename[split_filename.index('news') + 1]\n",
    "    return Counter(result), [article['url'], media, article['date'], article['category'], counter_to_string(Counter(result))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_to_string(counter):\n",
    "    word_freq_tup_pairs = counter.most_common()\n",
    "    word_freq_str_pairs = ['%s:%d' % (word, freq) for word, freq in word_freq_tup_pairs]\n",
    "    return '/'.join(word_freq_str_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_counter = Counter()\n",
    "csv_rows = []\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=16) as executor:\n",
    "    for idx, (counter, csv_row) in \\\n",
    "        enumerate(executor.map(find_cn_words_from_filename, filenames, chunksize=30000), 1):\n",
    "        freq_counter += counter\n",
    "        csv_rows.append(csv_row)\n",
    "        if idx % 10000 == 0:\n",
    "            print('Has completed %d tasks' % idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_counter.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "os.makedirs('../result', exist_ok=True)\n",
    "with open('../result/article_words.csv', 'w+') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['URL', 'Media', 'Date', 'Category', 'Word Frequency'])\n",
    "    \n",
    "    for csv_row in csv_rows:\n",
    "        writer.writerow(csv_row)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
