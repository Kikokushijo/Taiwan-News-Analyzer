{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from tenacity import retry, stop_after_attempt\n",
    "import simplejson as json\n",
    "from selenium import webdriver\n",
    "\n",
    "from utils import ArticleMeta, NewsCrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EttodayNewsCrawler(NewsCrawler):\n",
    "    \n",
    "    def date_to_newslist_url(self, date):\n",
    "        \n",
    "        return \"https://www.ettoday.net/news/news-list-%s-0.htm\" % date\n",
    "    \n",
    "    def unfold_abbr_newslink(self, url):\n",
    "        \n",
    "        return \"https://www.ettoday.net\" + url\n",
    "    \n",
    "    def newslink_generator(self):\n",
    "        \n",
    "        ettoday_date_form = re.compile(\"[0-9]{4}/[0-9]{2}/[0-9]{2} [0-9]{2}:[0-9]{2}\")\n",
    "        \n",
    "        for date in self.date_generator():\n",
    "            newslist_url = self.date_to_newslist_url(date)\n",
    "            newslist = self.get_bsObj_scroll_down(newslist_url)\n",
    "            news_datetimes = newslist.find_all('span', class_='date', text=ettoday_date_form)\n",
    "\n",
    "            for news_datetime in news_datetimes:\n",
    "                newslinks = news_datetime.parent.a['href']\n",
    "                yield newslinks\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(0),\n",
    "           retry_error_callback=lambda x: None)\n",
    "    def parse_category(self, newspage):\n",
    "        return newspage.findAll('a', itemprop='item')[1].span.text\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(0),\n",
    "           retry_error_callback=lambda x: None)\n",
    "    def parse_title(self, newspage):\n",
    "        return newspage.find('h1', class_='title', itemprop='headline').text\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(0),\n",
    "           retry_error_callback=lambda x: None)\n",
    "    def parse_article(self, newspage):\n",
    "        paragraphs = []\n",
    "        for paragraph in newspage.find('article').find_all('p'):\n",
    "            if paragraph.findChild():\n",
    "                continue\n",
    "            else:\n",
    "                paragraphs.append(paragraph.text)\n",
    "        return '\\n'.join(paragraphs)\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(0),\n",
    "           retry_error_callback=lambda x: (None, None))\n",
    "    def parse_date_time(self, newspage):\n",
    "        # workaround\n",
    "        datetime_string = newspage.find('time')['datetime'][:-6]\n",
    "        dt = datetime.datetime.strptime(datetime_string, \"%Y-%m-%dT%H:%M:%S\")\n",
    "        return str(dt.date()), str(dt.time())\n",
    "    \n",
    "    def is_valid_newspage(self, bsObj):\n",
    "        \n",
    "        if bsObj is None:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            if bsObj.find('em').text == '404錯誤':\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        except:\n",
    "            return True\n",
    "    \n",
    "    def saved_filename(self, url):\n",
    "        return url.split('/')[-1].split('.')[0] + '.json'\n",
    "    \n",
    "    def crawl_and_save(self):\n",
    "        \n",
    "        for newslink in self.newslink_generator():\n",
    "            article = self.get_page_attribute_from_link(self.unfold_abbr_newslink(newslink))\n",
    "            \n",
    "            if article is None:\n",
    "                continue\n",
    "\n",
    "            self.save_article_meta(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ettoday_crawler = EttodayNewsCrawler(\n",
    "    output_dir='../news/ettoday',\n",
    "    total_days=1,\n",
    "    start_date=datetime.date(year=2019, month=7, day=29)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ettoday_crawler.crawl_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = ettoday_crawler.get_page_attribute_from_link('https://www.ettoday.net/news/20190615/1467832.htm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
