{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import contextlib\n",
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from tenacity import retry, stop_after_attempt, wait_random\n",
    "import simplejson as json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "from utils import ArticleMeta, NewsCrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NownewsCrawler(NewsCrawler):\n",
    "    \n",
    "    # 3811581 is a piece of news at 2018/12/10 01:10:00, I start to crawl\n",
    "    # <link rel=\"shortlink\" href=\"https://www.nownews.com/?p=3787634\">\n",
    "    def __init__(self, output_dir, total_days, start_date=datetime.date.today(), start_id=3811581):\n",
    "        super(NownewsCrawler, self).__init__(output_dir, total_days, start_date)\n",
    "        self.start_id = start_id\n",
    "        self.end_date = start_date - datetime.timedelta(days=total_days)\n",
    "        self.redirected_url_checker = re.compile(r'^https:\\/\\/www.nownews.com\\/news\\/[0-9]{8}\\/[0-9]*\\/$')\n",
    "    \n",
    "    def newslink_generator(self):\n",
    "        \n",
    "        news_prefix = \"https://www.nownews.com/?p=\"\n",
    "        for news_id in range(self.start_id, 0, -1):\n",
    "            yield news_prefix + str(news_id)\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(3),\n",
    "           wait=wait_random(min=1, max=2),\n",
    "           retry_error_callback=lambda x: None)\n",
    "    def get_bsObj_check_redirected(self, url, url_check_func):\n",
    "\n",
    "        req = self.session.get(url, headers=self.headers)\n",
    "        if url_check_func(req.url, url):\n",
    "            return None\n",
    "        \n",
    "        second_url = req.url\n",
    "        req = requests.get(second_url)\n",
    "        if url_check_func(req.url, url):\n",
    "            print(req.url, second_url)\n",
    "            return None\n",
    "        \n",
    "        bsObj = BS(req.text, \"html.parser\")\n",
    "        return bsObj\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(0),\n",
    "           retry_error_callback=lambda x: None)\n",
    "    def parse_category(self, newspage):\n",
    "        return newspage.find('span', class_='td-bred-no-url-last').text\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(0),\n",
    "           retry_error_callback=lambda x: None)\n",
    "    def parse_title(self, newspage):\n",
    "        return newspage.find('h1', class_='entry-title').text\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(0),\n",
    "           retry_error_callback=lambda x: None)\n",
    "    def parse_article(self, newspage):\n",
    "        paragraphs = []\n",
    "        for paragraph in newspage.find('span', itemprop='articleBody').findAll('p'):\n",
    "            if paragraph.findChild() or paragraph.attrs or not paragraph.text:\n",
    "                continue\n",
    "            else:\n",
    "                paragraphs.append(paragraph.text)\n",
    "        return '\\n'.join(paragraphs)\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(0),\n",
    "           retry_error_callback=lambda x: (None, None))\n",
    "    def parse_date_time(self, newspage):\n",
    "        # workaround\n",
    "        datetime_string = newspage.find('time', class_=\"entry-date updated td-module-date\").text\n",
    "        dt = datetime.datetime.strptime(datetime_string, \"%Y-%m-%d %H:%M:%S\")\n",
    "        return str(dt.date()), str(dt.time())\n",
    "    \n",
    "    def is_valid_newspage(self, bsObj):\n",
    "        \n",
    "        if bsObj is None:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            if bsObj.findAll('body', class_='error404'):\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        except:\n",
    "            return True\n",
    "    \n",
    "    def saved_filename(self, url):\n",
    "        return url.split('=')[-1] + '.json'\n",
    "    \n",
    "    def crawl_and_save(self):\n",
    "        \n",
    "        num_old_news = 0\n",
    "        \n",
    "        for newslink in self.newslink_generator():\n",
    "\n",
    "            article = self.get_page_attribute_from_link(\n",
    "                newslink,\n",
    "                lambda x: (self.get_bsObj_check_redirected(x, lambda x, y: x.split('/')[-2] != y.split('=')[-1]))\n",
    "            )\n",
    "            \n",
    "            if article is None:\n",
    "                continue\n",
    "            \n",
    "            # check if there are continuous 100 piece of old news\n",
    "            if article.date <= str(self.end_date):\n",
    "                num_old_news += 1\n",
    "                if num_old_news >= 100:\n",
    "                    break\n",
    "            else:\n",
    "                num_old_news = 0\n",
    "        \n",
    "            self.save_article_meta(article)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = NownewsCrawler(output_dir='../news/nownews', total_days=1095, start_id=3767932)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../logs/nownews.txt', 'a') as f:\n",
    "#     with contextlib.redirect_stdout(f):\n",
    "crawler.crawl_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
